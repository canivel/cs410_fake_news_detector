{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import wordbatch\n",
    "from wordbatch.extractors import WordHash, WordBag, WordSeq, WordVec\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alphanums = re.compile('[\\W+]')\n",
    "nums_re= re.compile(\"\\W*[0-9]+\\W*\")\n",
    "triples_re= re.compile(r\"(\\w)\\1{2,}\")\n",
    "trash_re= [re.compile(\"<[^>]*>\"), re.compile(\"[^a-z0-9' -]+\"), re.compile(\" [.0-9'-]+ \"), re.compile(\"[-']{2,}\"),\n",
    "re.compile(\" '\"),re.compile(\"  +\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.DataFrame.from_csv(\"data/Tweets.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_attags= re.compile(\" @[^ ]* \")\n",
    "re_spaces= re.compile(\"\\w+]\")\n",
    "df['text']= df['text'].apply(lambda x: re_spaces.sub(\" \",re_attags.sub(\" \", \" \"+x+\" \"))[1:-1])\n",
    "df= df.drop_duplicates(subset=['text'])\n",
    "df.index= df['id']= range(df.shape[0])\n",
    "\n",
    "non_alphanums=re.compile('[^A-Za-z]+')\n",
    "def normalize_text(text): \n",
    "    return non_alphanums.sub(' ', text).lower().strip()\n",
    "\n",
    "df['text_normalized']= df['text'].map(lambda x: normalize_text(x))\n",
    "\n",
    "df= df.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text= text.lower()\n",
    "    text= nums_re.sub(\" NUM \", text)\n",
    "    text= \" \".join([word for word in non_alphanums.sub(\" \",text).strip().split() if len(word)>1])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = HashingVectorizer(decode_error= 'ignore', n_features=2**8, \n",
    "                               non_negative=False, ngram_range=(1,2), norm='l2')\n",
    "start = time.time()\n",
    "X = vectorizer.fit_transform(df['text_normalized'])\n",
    "print (\"Process time: {}\".format(time.time() - start))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wb = wordbatch.WordBatch(normalize_text,\n",
    "                             extractor=(WordHash, {\"decode_error\":'ignore', \"n_features\":2 ** 8,\n",
    "                                                   \"non_negative\":False, \"ngram_range\":(1,2), \"norm\":'l2'}\n",
    "                                       )\n",
    "                        , procs=8\n",
    "                        , method=\"serial\"\n",
    "                        )\n",
    "\n",
    "\n",
    "Xwb = wb.fit_transform(df['text_normalized'].values)\n",
    "print (\"Process time: {}\".format(time.time() - start))\n",
    "print(Xwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
